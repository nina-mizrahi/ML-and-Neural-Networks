{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Homework 10**"
      ],
      "metadata": {
        "id": "FZ0um49rzsKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start with a few imports and our running library of functions."
      ],
      "metadata": {
        "id": "7pR7ZPp9zzik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def TrainTestSplit(X,y,p,seed=1):\n",
        "  '''Splits feature matrix X and target array y into train and test sets\n",
        "  p is the fraction going to train'''\n",
        "  np.random.seed(seed) #controls randomness\n",
        "  size=len(y)\n",
        "  train_size=int(p*size)\n",
        "  train_mask=np.zeros(size,dtype=bool)\n",
        "  train_indices=np.random.choice(size, train_size, replace=False)\n",
        "  train_mask[train_indices]=True\n",
        "  test_mask=~train_mask\n",
        "  X_train=X[train_mask]\n",
        "  X_test=X[test_mask]\n",
        "  y_train=y[train_mask]\n",
        "  y_test=y[test_mask]\n",
        "  return X_train,X_test,y_train,y_test\n",
        "\n",
        "def PolyFeatures(x,d):\n",
        "  X=np.zeros((len(x),d+1))\n",
        "  for i in range(d+1):\n",
        "    X[:,i]=x**i\n",
        "  return X\n",
        "\n",
        "def AddOnes(X):\n",
        "  return np.concatenate((X,np.ones((len(X),1))),axis=1)\n",
        "\n",
        "class Scaler:\n",
        "  def __init__(self,z):\n",
        "    self.min=np.min(z,axis=0)\n",
        "    self.max=np.max(z,axis=0)\n",
        "\n",
        "  def scale(self,x):\n",
        "    return (x-self.min)/(self.max-self.min)\n",
        "\n",
        "  def unscale(self,x):\n",
        "    return x*(self.max-self.min)+self.min\n",
        "\n",
        "def train(X,y,max_iter,lr):\n",
        "  '''MSE minimization by Gradient Descent'''\n",
        "  X=np.array(X) #Just in case X is a DataFrame\n",
        "  y=np.array(y) #Just in case y is a Series\n",
        "  n=len(X)\n",
        "  coeff=np.ones(X.shape[1]) #Initialize all coeff to be 1 (something to play with?)\n",
        "  for i in range(max_iter):\n",
        "    resid=X@coeff-y\n",
        "    gradient=((X.T)@resid)/n #Lot's of lin alg here. Try to unpack it!\n",
        "    coeff=coeff-lr*gradient #Gradient Descent step.\n",
        "  return coeff\n",
        "\n",
        "def SGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):\n",
        "  '''Stochastic Gradient Descent With L1 and L2 regularization'''\n",
        "  #alpha=amount of L1 (Lasso) regularization\n",
        "  #beta=amount of L2 (Ridge) regularization\n",
        "  X=np.array(X) #Just in case X is a DataFrame\n",
        "  y=np.array(y) #Just in case y is a Series\n",
        "  n=len(X)\n",
        "  coeff=np.ones(X.shape[1]) #Initialize all coeff to be 1 (something to play with?)\n",
        "  indices=np.arange(len(X))\n",
        "  for i in range(epochs):\n",
        "    np.random.seed(i)\n",
        "    np.random.shuffle(indices)\n",
        "    X_shuffle=X[indices]\n",
        "    y_shuffle=y[indices]\n",
        "    num_batches=n//batch_size\n",
        "    for j in range(num_batches):\n",
        "      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      resid=X_batch@coeff-y_batch\n",
        "      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient #Gradient Descent step.\n",
        "    if n%batch_size!=0: #Check if there is a smaller leftover batch\n",
        "      X_batch=X_shuffle[num_batches*batch_size:] #last batch\n",
        "      y_batch=y_shuffle[num_batches*batch_size:] #last batch\n",
        "      resid=X_batch@coeff-y_batch\n",
        "      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient\n",
        "  return coeff\n",
        "\n",
        "def predict(X,coeff): #If X was scaled, then this will return scaled predictions\n",
        "  return X@coeff\n",
        "\n",
        "def MSE(pred,y):\n",
        "  return np.sum((pred-y)**2)/len(y)"
      ],
      "metadata": {
        "id": "o3bG8pKfzy4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we'll load the iris dataset as a pandas DataFrame."
      ],
      "metadata": {
        "id": "MuPrvVge0Gb5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "iris=(pd.read_csv('https://vincentarelbundock.github.io/Rdatasets/csv/datasets/iris.csv',index_col=0))"
      ],
      "metadata": {
        "id": "AibXEthd0F73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment you'll create a general Logistic Regression funcation, and train it to predict which flowers are virginicas just from their petal length and width. Since this is a very simple dataset, we won't worry about doing a train/test split, scaling, etc.\n",
        "\n",
        "To begin, we select the `Petal.Length` and `Petal.Width` columns to be our features `X`, and for our target `y` we'll create a boolean array which is True if the species is virginica, and False otherwise."
      ],
      "metadata": {
        "id": "tsojVh_CkD0g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array(iris[['Petal.Length','Petal.Width']])\n",
        "y=np.array(iris['Species']=='virginica')"
      ],
      "metadata": {
        "id": "tcOb3sPWkDWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, define a function `sigmoid` by the following formula:\n",
        "$$\\sigma(t)=\\frac{1}{1+e^{-t}}$$\n",
        "\n",
        "Be sure to use np.exp, and not math.exp, so that all operations are vectorized."
      ],
      "metadata": {
        "id": "27vFXND6mQNt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(t):\n",
        "  h = 1/(1+np.exp(-t))\n",
        "  return h"
      ],
      "metadata": {
        "id": "mOufNgpBmlPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What we are after is to find values of m, n, and b so that $$\\sigma(m*length+n*width+b)$$ is a good predictor of the probability of a flower being a virginica.\n",
        "\n",
        "In general, for a feature matrix `X` (with a column of ones), we want to find a vector of coefficients `coeff` so that $\\sigma$(X@coeff) is a good predictor of the target column.   \n",
        "\n",
        "Define a function `proba` that gives the value of this probability."
      ],
      "metadata": {
        "id": "PUjbGgjFmpPD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def proba(X,coeff):\n",
        "  g = sigmoid((X@coeff))\n",
        "  return g"
      ],
      "metadata": {
        "id": "FfckWF6InUkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use a logistic model defined by m=-2, n=0.5, and b=10 to generate an array of probabilities for the matrix X of petal lengths and widths (don't forget to add a column of ones!)."
      ],
      "metadata": {
        "id": "YA99XHRM5WpY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = AddOnes(X)\n",
        "coeff = np.array([-2,0.5,10])\n",
        "probs=proba(X, coeff)\n",
        "probs\n"
      ],
      "metadata": {
        "id": "IGPLB12q5aHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2146f6bf-ce6f-4ae8-d1fb-c0ef786a669d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.99932492, 0.99932492, 0.99944722, 0.99917558, 0.99932492,\n",
              "       0.99888746, 0.99935782, 0.99917558, 0.99932492, 0.99913334,\n",
              "       0.99917558, 0.99899323, 0.99929033, 0.9996104 , 0.99954738,\n",
              "       0.99925397, 0.9994998 , 0.99935782, 0.99883049, 0.99921575,\n",
              "       0.9987706 , 0.99925397, 0.99969655, 0.99894167, 0.99816706,\n",
              "       0.99899323, 0.99908895, 0.99917558, 0.99932492, 0.99899323,\n",
              "       0.99899323, 0.99925397, 0.99913334, 0.99932492, 0.99917558,\n",
              "       0.99954738, 0.99944722, 0.99929033, 0.99944722, 0.99917558,\n",
              "       0.99947417, 0.99947417, 0.99944722, 0.99917558, 0.9983412 ,\n",
              "       0.99935782, 0.99899323, 0.99932492, 0.99917558, 0.99932492,\n",
              "       0.78583498, 0.8519528 , 0.72111518, 0.93401099, 0.82491373,\n",
              "       0.83889105, 0.80218389, 0.98015969, 0.80999843, 0.94784644,\n",
              "       0.97068777, 0.91293423, 0.92414182, 0.78583498, 0.96923114,\n",
              "       0.86989153, 0.8519528 , 0.90887704, 0.8519528 , 0.93991335,\n",
              "       0.78583498, 0.93401099, 0.72111518, 0.76852478, 0.88594762,\n",
              "       0.86989153, 0.75026011, 0.70056714, 0.8519528 , 0.97068777,\n",
              "       0.95026349, 0.95689275, 0.94267582, 0.64565631, 0.8519528 ,\n",
              "       0.85814894, 0.79412963, 0.8641271 , 0.92056145, 0.93401099,\n",
              "       0.85814894, 0.81757448, 0.93086158, 0.98015969, 0.90465054,\n",
              "       0.90024951, 0.90465054, 0.88594762, 0.98954329, 0.92056145,\n",
              "       0.3208213 , 0.6791787 , 0.3208213 , 0.42555748, 0.37754067,\n",
              "       0.10433122, 0.8641271 , 0.15446527, 0.33181223, 0.27888482,\n",
              "       0.68997448, 0.58661758, 0.5124974 , 0.73105858, 0.73105858,\n",
              "       0.63413559, 0.47502081, 0.09112296, 0.06598901, 0.6791787 ,\n",
              "       0.4378235 , 0.76852478, 0.0831727 , 0.75026011, 0.41338242,\n",
              "       0.24973989, 0.78583498, 0.75026011, 0.46257015, 0.31002552,\n",
              "       0.22270014, 0.14185106, 0.47502081, 0.63413559, 0.37754067,\n",
              "       0.2592251 , 0.5       , 0.47502081, 0.78583498, 0.5621765 ,\n",
              "       0.5       , 0.72111518, 0.6791787 , 0.34298954, 0.46257015,\n",
              "       0.6791787 , 0.72111518, 0.64565631, 0.58661758, 0.66818777])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function `predict` that takes an array of probabilities and returns True for each one that is above 0.5, and False otherwise. Then, use this function to make predictions on weather or not each flower is a virginica, based on the logistic model with coefficients m=-2, n=0.5, and b=10."
      ],
      "metadata": {
        "id": "dI72dhoE5yCx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(probs):\n",
        "  t = (probs > 0.5)\n",
        "  return t\n",
        "\n",
        "predictions=predict(probs)\n",
        "predictions"
      ],
      "metadata": {
        "id": "ZCQCra4X58HJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66e1e9df-da76-4891-8bce-9f92b169cbc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
              "        True, False,  True, False, False, False, False,  True, False,\n",
              "       False, False,  True,  True,  True,  True,  True,  True, False,\n",
              "       False, False,  True, False,  True, False,  True, False, False,\n",
              "        True,  True, False, False, False, False, False,  True, False,\n",
              "       False, False, False,  True,  True, False,  True,  True, False,\n",
              "       False,  True,  True,  True,  True,  True])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function `accuracy` that takes an array or predicted True/False values and an array of target True/False values, and determines the accuracy of the predictions. Then, use this to determine the accuracy of the logistic model with coefficients m=-2, n=0.5, and b=10."
      ],
      "metadata": {
        "id": "7x-eNtYjoi46"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def accuracy(pred,y):\n",
        "  num_correct = np.sum(pred == y)\n",
        "  total=len(y)\n",
        "  acc = num_correct / total\n",
        "  return acc\n",
        "\n",
        "acc=accuracy(predictions,y)\n",
        "acc"
      ],
      "metadata": {
        "id": "jb0YlNK56pYi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "696e4a87-9c1e-4777-f09e-f331c9ff5cb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15333333333333332"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function we will optimize is not accuracy, but rather the log of the probability that our guesses for the target column are correct, if we guess according to the probabilities given by our model (log-loss). To compute this, define a function that calculates the *mean* log loss. Then, apply this function to the logistic model with coefficients m=-2, n=0.5, and b=10. As discussed in class, the log loss is given by:\n",
        "\n",
        "$$\\log \\left(\\prod \\limits _{target_i=T} p_i \\cdot \\prod \\limits _{target_i=F} (1-p_i)\\right)\\\\\n",
        "=\\sum \\limits _{target_i=T} \\log(p_i) + \\sum \\limits _{target_i=F} \\log(1-p_i)\\\\\n",
        "=\\sum \\limits _i  target_i \\log(p_i) +  (1-target_i)\\log(1-p_i)$$\n",
        "\n",
        "As in the case of RSS vs MSE, the mean log loss is obtained from the log loss by dividing by the number of rows in our dataset, to account for datasets of varying size."
      ],
      "metadata": {
        "id": "tM57Y1dYpY3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LogLoss(probs,y):\n",
        "  l = sum((np.log(probs)*y)+(1-y)*(np.log(1-probs)))/len(y)\n",
        "  return l\n",
        "\n",
        "LL=LogLoss(probs,y)\n",
        "LL"
      ],
      "metadata": {
        "id": "NePpvEVBrzBE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7edfe82-061d-4eed-959b-84470261c3c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-3.4500448158079577"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above quantity is a number between $-\\infty$ and $0$, with numbers closer to 0 being better. Rather than maximize this, we will minimze its negation (i.e. we'll put a negative sign in front and use gradient descent).\n",
        "\n",
        "For example, in the case of our iris model, the gradient of the mean log loss is given by the three partial derivatives:\n",
        "$$\\frac{\\partial ll}{\\partial m}= \\frac{1}{len(y)}\\sum \\limits _i (p_i-target_i)length_i$$\n",
        "$$\\frac{\\partial ll}{\\partial n}= \\frac{1}{len(y)}\\sum \\limits _i (p_i-target_i)width_i$$\n",
        "$$\\frac{\\partial ll}{\\partial b}= \\frac{1}{len(y)}\\sum \\limits _i (p_i-target_i)$$\n",
        "\n",
        "Again, we will also divide these by the number of rows in our dataset, to account for datasets of varying size."
      ],
      "metadata": {
        "id": "BRSN9ovRr2LI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, if you have a feature matrix X and a logistic model giving probabilities `proba` for a target `y` then the gradient vector is given by `X.T@(proba-y)/len(X)`.\n",
        "\n",
        "Use this to modify the SGD function defined in the previous homework assignment to find an optimal Logistic regression model. As before, we will use stochastic (batch) gradient descent, with possible L1 and L2 regularization."
      ],
      "metadata": {
        "id": "19Czn6FbwuR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def LogisticSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):\n",
        "  '''Stochastic Gradient Descent for Logistic Regression'''\n",
        "  #alpha=amount of L1 (Lasso) regularization\n",
        "  #beta=amount of L2 (Ridge) regularization\n",
        "  X=np.array(X) #Just in case X is a DataFrame\n",
        "  y=np.array(y) #Just in case y is a Series\n",
        "  n=len(X)\n",
        "  coeff=np.ones(X.shape[1]) #Initialize all coeff to be 1 (something to play with?)\n",
        "  indices=np.arange(len(X))\n",
        "  for i in range(epochs):\n",
        "    np.random.seed(i)\n",
        "    np.random.shuffle(indices)\n",
        "    X_shuffle=X[indices]\n",
        "    y_shuffle=y[indices]\n",
        "    num_batches=n//batch_size\n",
        "    for j in range(num_batches):\n",
        "      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      y_pred = proba(X_batch, coeff)\n",
        "      resid=y_pred-y_batch\n",
        "      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient #Gradient Descent step.\n",
        "    if n%batch_size!=0: #Check if there is a smaller leftover batch\n",
        "      X_batch=X_shuffle[num_batches*batch_size:] #last batch\n",
        "      y_batch=y_shuffle[num_batches*batch_size:] #last batch\n",
        "      y_pred = proba(X_batch, coeff)\n",
        "      resid=y_pred-y_batch\n",
        "      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient\n",
        "  return coeff"
      ],
      "metadata": {
        "id": "jLJK55eU0o0Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use this function to compute coefficients for a logistic model for the feature matrix X (with a columns of ones added) and target y. Run your code for 2000 epochs, with batch sizes of 50, and learning rate 0.01. For this assignment, we won't worry about regularization, so leave alpha and beta at 0."
      ],
      "metadata": {
        "id": "_yp0AqJOm7PH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "newcoeff=LogisticSGD(X,y,2000,50,0.01)\n",
        "#newcoeff=LogisticSGD(X,y,epochs=2000,batch_size=50,lr=0.01,alpha=0,beta=0)\n",
        "newcoeff"
      ],
      "metadata": {
        "id": "o_0bEXPD9uyd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b8b39ef-69b0-4fc2-86ed-36a464a3da87"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.17670254,  2.73978481, -3.55043789])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculate the accuracy of the logistic model you found with these coefficients."
      ],
      "metadata": {
        "id": "U1SCskJXnLAS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#X_with_intercept = np.hstack((np.ones((len(X), 1)), X))\n",
        "#probs = proba(X, newcoeff)\n",
        "#pred_labels = (probs >= 0.5).astype(int)\n",
        "#newaccuracy = np.mean(pred_labels == y)\n",
        "#newaccuracy\n",
        "\n",
        "#probs1 = proba(X,newcoeff)\n",
        "#predictions1=predict(probs1)\n",
        "newaccuracy=accuracy(predictions1,y)\n",
        "newaccuracy"
      ],
      "metadata": {
        "id": "l_QH7DV3t4WF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20eace68-1a4f-4d80-fa4d-cab1bf9dcd52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9466666666666667"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the \"decision boundary\" between where your model will predict virginica (in blue) vs the other species, run the following code block:"
      ],
      "metadata": {
        "id": "d-_Vobu0HzK_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def func(x):\n",
        "  m=-newcoeff[0]/newcoeff[1]\n",
        "  b=-newcoeff[2]/newcoeff[1]\n",
        "  return m*x+b\n",
        "\n",
        "cdict={'versicolor':'r','setosa':'g','virginica':'b'}\n",
        "colors=iris.apply(lambda x:cdict[x.Species],axis=1)\n",
        "\n",
        "plt.scatter(X[:,0],X[:,1],c=colors)\n",
        "plt.plot(X[:,0],func(X[:,0]))"
      ],
      "metadata": {
        "id": "JYnGnyk_zEIq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "a568729d-fe5a-4dff-9933-0bd1d287d6f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc6f619e8e0>]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}