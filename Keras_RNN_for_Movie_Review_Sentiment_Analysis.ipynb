{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Homework 19**"
      ],
      "metadata": {
        "id": "QsqtD65bt_XD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start by loading the imdb movie review dataset that is packaged with keras. We'll only load the first 100 words of each review (if they are longer), and tell keras to restrict to a 10,000 word vocabulary. The target variable y will be an array of 0's and 1's, indicating a positive or negative review.  "
      ],
      "metadata": {
        "id": "LdM_EK8hhZBU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "(Xtrain,ytrain),(Xtest,ytest)=imdb.load_data(maxlen=100,num_words=10000,index_from=0)"
      ],
      "metadata": {
        "id": "pPqrGaF4NHsf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset also comes with a dictionary for tokenization. The keys for this dictionary are the words in the vocabulary, and the values are the numbers assigned to each word (just like the `tokens` dictionary attribute of your `Tokenizer` class from the last assignment). We import this dictionary as `index`, and look at the first 10 key/value pairs:"
      ],
      "metadata": {
        "id": "LVCMyEN9iBfJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "index=imdb.get_word_index()\n",
        "list(index.items())[:10]"
      ],
      "metadata": {
        "id": "wnPbw6xgNbEo",
        "outputId": "ec03287d-ca21-4994-e1eb-7712ca909597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('fawn', 34701),\n",
              " ('tsukino', 52006),\n",
              " ('nunnery', 52007),\n",
              " ('sonja', 16816),\n",
              " ('vani', 63951),\n",
              " ('woods', 1408),\n",
              " ('spiders', 16115),\n",
              " ('hanging', 2345),\n",
              " ('woody', 2289),\n",
              " ('trawling', 52008)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that Xtrain[0] is not the words of the first movie review of the dataset, but rather the indices of those words. To see the review itself, we have to convert from indices back to words. To this end, we'll build a dictionary word_from_index whose keys are indices and values are corresponding words."
      ],
      "metadata": {
        "id": "JveUPp1hlRSp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_from_index={}\n",
        "for word in index:\n",
        "  words_from_index[index[word]]=word"
      ],
      "metadata": {
        "id": "caQIPzAQlqxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now look at review 1:"
      ],
      "metadata": {
        "id": "dp3r_0nUnxDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "review1=''\n",
        "for n in Xtrain[1]:\n",
        "  review1+=words_from_index[n]+' '\n",
        "review1"
      ],
      "metadata": {
        "id": "zc2QoKSoOsqc",
        "outputId": "0fdcd320-0bff-4518-8ea0-83d00820597b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"the when i rented this movie i had very low expectations but when i saw it i realized that the movie was less a lot less than what i expected the actors were bad the doctor's wife was one of the worst the story was so stupid it could work for a disney movie except for the murders but this one is not a comedy it is a laughable masterpiece of stupidity the title is well chosen except for one thing they could add stupid movie after dead husbands i give it 0 and a half out of 5 \""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now Xtrain is a 1-dimensional numpy array of python lists. To convert it to a 2-dimensional numpy array, we'll need every every element of Xtrain to be a list of the same length. Write a function that takes a list-of-lists, and adds an appropriate number of 0's to each if their length is less than 100. (This is called *padding*.)"
      ],
      "metadata": {
        "id": "ob0wuGBKoXCT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pad(X):\n",
        "  '''X is a list of lists of all different lengths\n",
        "  function should return a list of lists of length 100 by padding with 0's'''\n",
        "  paddedX = []\n",
        "  for i in X:\n",
        "    if len(i)<100:\n",
        "      padding = i + [0] * (100 - len(i))\n",
        "    else:\n",
        "      padding = i[:100]\n",
        "    paddedX.append(padding)\n",
        "  return paddedX"
      ],
      "metadata": {
        "id": "u6elT9Paqy8r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll now use this function to convert Xtrain and Xtest to 2D numpy arrays. (At the same time we convert ytrain and ytest to numpy arrays as well.)"
      ],
      "metadata": {
        "id": "ITTlcuH0vLM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "Xtrain=np.array([np.array(l) for l in pad(Xtrain)])\n",
        "ytrain=np.array(ytrain)\n",
        "Xtest=np.array([np.array(l) for l in pad(Xtest)])\n",
        "ytest=np.array(ytest)\n"
      ],
      "metadata": {
        "id": "6bf-zDt4RX-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ytrain and ytest are now 1-D numpy arrays of 0's and 1's. Import the keras function you need, and convert these to 2D one-hot encodings."
      ],
      "metadata": {
        "id": "3gKp9pdJvaxU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "ytrain=to_categorical(ytrain)\n",
        "ytest=to_categorical(ytest)"
      ],
      "metadata": {
        "id": "ljj1MkV1sXmN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework we introduce the Embedding layer, which has the same effect as a one-hot encoding followed by a dense layer. We import this here, as well as a GRU layer that you can use in place of SimpleRNN. *Import all the other layers you'll need to build a recurrent Neural Network to predict ytrain from Xtrain.*"
      ],
      "metadata": {
        "id": "RVtEowCvvzUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import GRU\n",
        "from tensorflow.keras.layers import Embedding\n",
        "\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import Softmax\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.models import Sequential\n"
      ],
      "metadata": {
        "id": "V3FrAVd8PAVe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Build your model! For the embedding layer, the input_dim should be the size of your vocabulary, and the output_dim should be about 1% of that (you can play with that number).   "
      ],
      "metadata": {
        "id": "cHGp06EmwVj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#vocab_size = 10000\n",
        "#maxlen = 100"
      ],
      "metadata": {
        "id": "geIcDHKYuByZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=10000, output_dim=100))\n",
        "model.add(GRU(32))\n",
        "model.add(Dense(2, activation='softmax'))\n",
        "\n",
        "#model = Sequential()\n",
        "#model.add(Embedding(input_dim=vocab_size, output_dim=100))\n",
        "#model.add(GRU(units=64, dropout=0.2, recurrent_dropout=0.2))\n",
        "#model.add(Dense(units=2, activation='softmax'))"
      ],
      "metadata": {
        "id": "r17asLlZQzkl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Look at it:"
      ],
      "metadata": {
        "id": "XIQeN_VtwafG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "2lxIfdY2TQL9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad11fecd-3373-4b34-a1d0-fba9f39aa846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_2 (Embedding)     (None, None, 100)         1000000   \n",
            "                                                                 \n",
            " gru_2 (GRU)                 (None, 32)                12864     \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 2)                 66        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,012,930\n",
            "Trainable params: 1,012,930\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compile** your model:"
      ],
      "metadata": {
        "id": "5p9kN05TwcJd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "jGozIgz-TSrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Fit*** your model to Xtrain and ytrain. 10 epochs should be enough."
      ],
      "metadata": {
        "id": "wBIps7BNwexw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(Xtrain, ytrain, epochs=10, batch_size=32) #validation_data=(Xtest, ytest)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XSJIhEZGuk2n",
        "outputId": "1e11ee88-fd5b-4e37-c707-e1b945fb175d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "87/87 [==============================] - 8s 68ms/step - loss: 0.6885 - accuracy: 0.5514\n",
            "Epoch 2/10\n",
            "87/87 [==============================] - 7s 83ms/step - loss: 0.6869 - accuracy: 0.5633\n",
            "Epoch 3/10\n",
            "87/87 [==============================] - 6s 69ms/step - loss: 0.6710 - accuracy: 0.5921\n",
            "Epoch 4/10\n",
            "87/87 [==============================] - 7s 84ms/step - loss: 0.5120 - accuracy: 0.7652\n",
            "Epoch 5/10\n",
            "87/87 [==============================] - 6s 68ms/step - loss: 0.2743 - accuracy: 0.8983\n",
            "Epoch 6/10\n",
            "87/87 [==============================] - 7s 84ms/step - loss: 0.1597 - accuracy: 0.9528\n",
            "Epoch 7/10\n",
            "87/87 [==============================] - 6s 68ms/step - loss: 0.1051 - accuracy: 0.9701\n",
            "Epoch 8/10\n",
            "87/87 [==============================] - 7s 83ms/step - loss: 0.0706 - accuracy: 0.9831\n",
            "Epoch 9/10\n",
            "87/87 [==============================] - 6s 68ms/step - loss: 0.0455 - accuracy: 0.9903\n",
            "Epoch 10/10\n",
            "87/87 [==============================] - 7s 82ms/step - loss: 0.0345 - accuracy: 0.9921\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fb980a7f4f0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate your model on Xtest and ytest. Your accuracy should be at least 80%."
      ],
      "metadata": {
        "id": "wAW7GeEbwrQS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy=model.evaluate(Xtest,ytest)[1]\n",
        "accuracy"
      ],
      "metadata": {
        "id": "D_xU0Ec1TzAz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ebf4c96-7ab8-4ea5-9f55-43d7eb5cba8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93/93 [==============================] - 2s 13ms/step - loss: 0.6621 - accuracy: 0.8140\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8140398263931274"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}