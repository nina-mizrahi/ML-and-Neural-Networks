{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Homework 11**"
      ],
      "metadata": {
        "id": "FZ0um49rzsKR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll start with a few imports and our running library of functions."
      ],
      "metadata": {
        "id": "7pR7ZPp9zzik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "#import matplotlib.pyplot as plt\n",
        "#from sklearn.datasets import load_digits\n",
        "\n",
        "def TrainTestSplit(X,y,p,seed=1):\n",
        "  '''Splits feature matrix X and target array y into train and test sets\n",
        "  p is the fraction going to train'''\n",
        "  np.random.seed(seed) #controls randomness\n",
        "  size=len(y)\n",
        "  train_size=int(p*size)\n",
        "  train_mask=np.zeros(size,dtype=bool)\n",
        "  train_indices=np.random.choice(size, train_size, replace=False)\n",
        "  train_mask[train_indices]=True\n",
        "  test_mask=~train_mask\n",
        "  X_train=X[train_mask]\n",
        "  X_test=X[test_mask]\n",
        "  y_train=y[train_mask]\n",
        "  y_test=y[test_mask]\n",
        "  return X_train,X_test,y_train,y_test\n",
        "\n",
        "def PolyFeatures(x,d):\n",
        "  X=np.zeros((len(x),d+1))\n",
        "  for i in range(d+1):\n",
        "    X[:,i]=x**i\n",
        "  return X\n",
        "\n",
        "def AddOnes(X):\n",
        "  return np.concatenate((X,np.ones((len(X),1))),axis=1)\n",
        "\n",
        "class Scaler:\n",
        "  def __init__(self,z):\n",
        "    self.min=np.min(z,axis=0)\n",
        "    self.max=np.max(z,axis=0)\n",
        "\n",
        "  def scale(self,x):\n",
        "\n",
        "    return (x-self.min)/(self.max-self.min+0.000001)\n",
        "\n",
        "  def unscale(self,x):\n",
        "    return x*(self.max-self.min+0.000001)+self.min\n",
        "\n",
        "\n",
        "def LinearSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):\n",
        "  '''Stochastic Gradient Descent With L1 and L2 regularization'''\n",
        "  #alpha=amount of L1 (Lasso) regularization\n",
        "  #beta=amount of L2 (Ridge) regularization\n",
        "  X=np.array(X) #Just in case X is a DataFrame\n",
        "  y=np.array(y) #Just in case y is a Series\n",
        "  n=len(X)\n",
        "  coeff=np.ones(X.shape[1]) #Initialize all coeff to be 1 (something to play with?)\n",
        "  indices=np.arange(len(X))\n",
        "  for i in range(epochs):\n",
        "    np.random.seed(i)\n",
        "    np.random.shuffle(indices)\n",
        "    X_shuffle=X[indices]\n",
        "    y_shuffle=y[indices]\n",
        "    num_batches=n//batch_size\n",
        "    for j in range(num_batches):\n",
        "      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      resid=X_batch@coeff-y_batch\n",
        "      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient #Gradient Descent step.\n",
        "    if n%batch_size!=0: #Check if there is a smaller leftover batch\n",
        "      X_batch=X_shuffle[num_batches*batch_size:] #last batch\n",
        "      y_batch=y_shuffle[num_batches*batch_size:] #last batch\n",
        "      resid=X_batch@coeff-y_batch\n",
        "      gradient=lr*((X_batch.T)@resid)/len(X_batch)+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient\n",
        "  return coeff\n",
        "\n",
        "def LinearPredict(X,coeff): #If X was scaled, then this will return scaled predictions\n",
        "  return X@coeff\n",
        "\n",
        "def MSE(pred,y):\n",
        "  return np.sum((pred-y)**2)/len(y)\n",
        "\n",
        "def sigmoid(t):\n",
        "  return 1/(1+np.exp(-t))\n",
        "\n",
        "def LogisticProbability(X,coeff):\n",
        "  return sigmoid(X@coeff)\n",
        "\n",
        "def LogisticPredict(X,coeff):\n",
        "  return X@coeff>0\n",
        "\n",
        "def accuracy(pred,y):\n",
        "  return np.sum(pred==y)/len(y)\n",
        "\n",
        "def LogLoss(probs,y):\n",
        "  return np.sum(y*np.log(probs)+(1-y)*np.log(1-probs))/len(y)\n",
        "\n",
        "def LogisticSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):\n",
        "  '''Stochastic Gradient Descent for Logistic Regression'''\n",
        "  #alpha=amount of L1 (Lasso) regularization\n",
        "  #beta=amount of L2 (Ridge) regularization\n",
        "  X=np.array(X)\n",
        "  y=np.array(y)\n",
        "  n=len(X)\n",
        "  coeff=np.ones(X.shape[1])\n",
        "  indices=np.arange(len(X))\n",
        "  for i in range(epochs):\n",
        "    np.random.seed(i)\n",
        "    np.random.shuffle(indices)\n",
        "    X_shuffle=X[indices]\n",
        "    y_shuffle=y[indices]\n",
        "    num_batches=n//batch_size\n",
        "    for j in range(num_batches):\n",
        "      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      probs=LogisticProbability(X_batch,coeff)\n",
        "      grad=X_batch.T@(probs-y_batch)/len(X_batch)\n",
        "      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient\n",
        "    if n%batch_size!=0: #Check if there is a smaller leftover batch\n",
        "      X_batch=X_shuffle[num_batches*batch_size:] #last batch\n",
        "      y_batch=y_shuffle[num_batches*batch_size:] #last batch\n",
        "      probs=LogisticProbability(X_batch,coeff)\n",
        "      grad=X_batch.T@(probs-y_batch)/len(X_batch)\n",
        "      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient\n",
        "  return coeff\n"
      ],
      "metadata": {
        "id": "o3bG8pKfzy4M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that takes a feature matrix `X` with shape (m,n), and a coefficient matrix `coeff` of shape (n,k), and returns an array with m elements that gives the predicted class of a softmax regression model. Here, m is the number of observations (rows of `X`), n is the number of features (columns of `X`), and k is the number of classes.  The `i`th element of the array that is returned should be the column number of the maximum element of the `i`th row of `X` times `coeff`."
      ],
      "metadata": {
        "id": "H1ViUUWMuQ9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SoftmaxPredict(X,coeff):\n",
        "  p = np.dot(X, coeff)\n",
        "  pred = np.argmax(p, axis=1)\n",
        "  return pred"
      ],
      "metadata": {
        "id": "2uGtWwMVP-Z-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function that takes a feature matrix `X` and a coefficient matrix `coeff` (as in the previous problem), and returns a matrix of predicted probabilities of shape (m,k). The entry in row `i`, column `j`, is the probability that the observation in row `i` of X is in class `j`. This is found by the formula\n",
        "$$\\frac{exp(s^j_i)}{\\sum \\limits _{j=1} ^k exp(s^j_i)},$$\n",
        "where $s^j_i$ is the entry in row `i`, column `j`, of the matrix that is the product of `X` and `coeff`. Note that this formula will always be a number between 0 and 1."
      ],
      "metadata": {
        "id": "ugCGjafMu_v1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SoftmaxProbability(X,coeff):\n",
        "  s = np.dot(X, coeff)\n",
        "  prob = np.exp(s)\n",
        "  return prob/np.sum(prob,axis=1,keepdims=True)"
      ],
      "metadata": {
        "id": "x27g7_DAJ7-h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a function `OneHot` that takes a target array `y` with m entries, where each entry is one of k possible classes, and returns a matrix of shape (m,k), where each entry is a 1 or a 0. If there is a 1 in row `i`, column `j`,  of OneHot(y), that would indicate that the `i`th entry of `y` is the number `j`."
      ],
      "metadata": {
        "id": "BOsSiwKXv2IC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def OneHot(y):\n",
        "  m = y.shape[0]\n",
        "  k = np.max(y) + 1\n",
        "  onehot = np.zeros((m, k))\n",
        "  onehot[np.arange(m), y] = 1\n",
        "  return onehot"
      ],
      "metadata": {
        "id": "U_w3fgucgZbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `SoftmaxSGD` finds the coefficients of softmax regression by gradient descent. The code is almost identical to the LogisticSGD function at the top of this colab. Read the code below, paying special attention ot the comments, and complete the two missing lines indicated by \"#YOUR CODE HERE\".\n",
        "\n",
        "Note that the gradient of the categorical cross-entropy function (the loss function used in Softmax regression) is given by\n",
        "$$X^{T}(probs-\\overline{y})$$\n",
        "where $probs$ is given by `SoftmaxProbability(X,coeff)` and $\\overline{y}$ is the One Hot enoding of the target vector $y$."
      ],
      "metadata": {
        "id": "O_UBX-6jwiSU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def SoftmaxSGD(X,y,epochs,batch_size,lr,alpha=0,beta=0):\n",
        "  '''Stochastic Gradient Descent for Softmax Regression'''\n",
        "  X=np.array(X)\n",
        "  y=np.array(y)\n",
        "  classes=np.max(y)+1 #THIS IS NEW!\n",
        "  ybar=OneHot(y) #THIS IS NEW!\n",
        "  n=len(X)\n",
        "  coeff=np.ones((X.shape[1],classes)) #initial coeff matrix. Note the shape!\n",
        "  indices=np.arange(len(X))\n",
        "  for i in range(epochs):\n",
        "    np.random.seed(i)\n",
        "    np.random.shuffle(indices)\n",
        "    X_shuffle=X[indices]\n",
        "    y_shuffle=ybar[indices] #NOTE THE CHANGE FROM y to ybar\n",
        "    num_batches=n//batch_size\n",
        "    for j in range(num_batches):\n",
        "      X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "      probs=SoftmaxProbability(X_batch, coeff)\n",
        "      grad=X_batch.T@(probs-y_batch)/len(X_batch) #NOTHING NEW HERE!!!!\n",
        "      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient\n",
        "    if n%batch_size!=0: #Check if there is a smaller leftover batch\n",
        "      X_batch=X_shuffle[num_batches*batch_size:]\n",
        "      y_batch=y_shuffle[num_batches*batch_size:]\n",
        "      probs=SoftmaxProbability(X_batch, coeff)\n",
        "      grad=X_batch.T@(probs-y_batch)/len(X_batch)\n",
        "      gradient=lr*grad+alpha*(2*(coeff>0)-1)+beta*coeff\n",
        "      coeff=coeff-gradient\n",
        "  return coeff\n"
      ],
      "metadata": {
        "id": "o578f5schYzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now load the `digits` feature matrix and target that we first encountered in Homework 4."
      ],
      "metadata": {
        "id": "SnoKOiSyw5le"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "X=load_digits().data\n",
        "y=load_digits().target"
      ],
      "metadata": {
        "id": "d1ulPgG1H25S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, do the following:\n",
        "1. 80/20 Test-train split to obtain Xtrain, Xtest, ytrain and ytest\n",
        "2. Scale Xtrain and Xtest\n",
        "3. Add a column of ones to Xtrain and Xtest"
      ],
      "metadata": {
        "id": "tCi4VyAnxD8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Xtrain, Xtest, ytrain, ytest = TrainTestSplit(X, y, 0.8, seed=1)\n",
        "scaler = Scaler(Xtrain)\n",
        "Xtrain = scaler.scale(Xtrain)\n",
        "Xtest = scaler.scale(Xtest)\n",
        "Xtrain = AddOnes(Xtrain)\n",
        "Xtest = AddOnes(Xtest)"
      ],
      "metadata": {
        "id": "zHwjJjnbxBi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find the coefficient matrix of a softmax regression model to predict which digit is which. Use 10000 epochs, with batch sizes of 5000, and a learning rate of 0.01."
      ],
      "metadata": {
        "id": "f03UPFwUxVIF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "coeff=SoftmaxSGD(Xtrain,ytrain,10000,5000,0.01,alpha=0,beta=0) #see if we need this bc didnt work with it last time\n",
        "coeff"
      ],
      "metadata": {
        "id": "Z_RRRAhsgTQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate predictions for your model on the test set Xtest."
      ],
      "metadata": {
        "id": "LJMY9kBaxqWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred=SoftmaxPredict(Xtest,coeff)\n",
        "pred"
      ],
      "metadata": {
        "id": "tHaSN_tsgV5U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the accuracy of those predictions. (You may use the `accuracy` function at the top of this colab.)"
      ],
      "metadata": {
        "id": "yGQ2yQh3Nka7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "acc=accuracy(pred,ytest)\n",
        "acc"
      ],
      "metadata": {
        "id": "CggbO6NmLv1h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95c02438-2ba8-4801-c57e-3960e5b07793"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9444444444444444"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ]
}