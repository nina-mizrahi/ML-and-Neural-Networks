{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxebqYTaL7Op"
      },
      "source": [
        "**Homework 18**\n",
        "\n",
        "In this assignment your will train a RNN to predict characters of *Alice in Wonderland*, from strings of consecutive characters.\n",
        "\n",
        "We begin as usual with the imports you will need for this assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vnuC9CayTbrH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import SimpleRNN\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import InputLayer\n",
        "from tensorflow.keras.layers import Softmax\n",
        "\n",
        "from tensorflow.keras.layers import LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6DcpEWAcsiWv"
      },
      "source": [
        "Run the following text block to read *Alice in Wonderland* from the web, store it in the variable `text`, convert to lower case and remove punctuation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lETC1jDaQrKp"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "from urllib.request import urlopen\n",
        "url='https://gist.githubusercontent.com/phillipj/4944029/raw/75ba2243dd5ec2875f629bf5d79f6c1e4b5a8b46/alice_in_wonderland.txt'\n",
        "text = urlopen(url).read().decode('utf-8')\n",
        "text=text.lower()\n",
        "text=[c for c in text if (c not in string.punctuation) and (c!='\\n')]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a class `Tokenizer` with the following methods:\n",
        "\n",
        "\n",
        "*   `__init__`, a method that builds a dictionary `tokens` whose keys are the set of unique characters in some input `text`, and values are integers.\n",
        "*   `encode`, a method that takes in a corpus of text, converts each character according to the dictionary built by the __init__ method, and outputs a list of those integers.\n",
        "*   `decode`, a method that takes a single integer (a value from the dictionary), and returns the corresponding character key.\n",
        "\n"
      ],
      "metadata": {
        "id": "68T7cjSLRMEb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Tokenizer():\n",
        "  def __init__(self,text):\n",
        "    #Build a dictionary of tokens\n",
        "    self.tokens = {}\n",
        "    for c in set(text):\n",
        "      self.tokens[c] = len(self.tokens)\n",
        "\n",
        "  def encode(self,text):\n",
        "    #Encode text using token dictionary, outputs list of those integers\n",
        "    encoded_text = [self.tokens[c] for c in text]\n",
        "    return encoded_text\n",
        "\n",
        "  def decode(self,n):\n",
        "    #Decode integer n to corresponding character\n",
        "    for c, i in self.tokens.items():\n",
        "      if i == n:\n",
        "        return c\n",
        "    return None\n",
        "\n"
      ],
      "metadata": {
        "id": "jBaSjQNWTEsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "```\n",
        "# This is formatted as code\n",
        "```\n",
        "\n",
        "Now, create an object called `tok` of your `Tokenizer` class, and use it to encode `text` as a list of integers, `text_indices`."
      ],
      "metadata": {
        "id": "nTBoQ2PaU9I6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tok=Tokenizer(text)\n",
        "text_indices=tok.encode(text)"
      ],
      "metadata": {
        "id": "lJnStdvpVEyw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For convenience, we'll define `n` to be the length of your tokenizer dictionary:"
      ],
      "metadata": {
        "id": "CMJOidgupSAY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n=len(tok.tokens)"
      ],
      "metadata": {
        "id": "t9rHu8C_pYFf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0F-VTPcsvCY"
      },
      "source": [
        "The next task is to create feature sequences and targets. From `text_indices`, create a list-of-lists `X`. Each sublist of `X` should correspond to 50 consecutive elements of `text_indices`. At the same time, create a list `y` which contains the indices of the characters that follow each sublist of `X`. For example, `X[0]` should be a list containing the first 50 elements of `text_indices`: `text_indices[0]` through `text_indices[49]`. `y[0]` should be the 51st element, `text_indices[50]`. Something very similar was done in Homework 17.\n",
        "\n",
        "To keep the size of the feature and target vectors manageable, consecutive lists in `X` should be shifted by 3, so the overlap is 47 elements. Hence, `X[1]` should be a list containing the integers `text_indices[3]` through `text_indices[52]`, and `y[1]` should be the integer `text_indices[53]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VJ3XPYLUTjbA"
      },
      "outputs": [],
      "source": [
        "X=[]\n",
        "y=[]\n",
        "for i in range(0,len(text_indices)-50-1,3):\n",
        "  X.append(text_indices[i:i+50])\n",
        "  y.append(text_indices[i+50])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5HcBpY_ut5T"
      },
      "source": [
        "Convert `X` and `y` to numpy arrays with the same names, and check their shapes. If done correctly, the shape of `X` should be (45539, 50) and the shape of `y` should be (45539, ):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tc1lvY5wUE_o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3dcc3dd0-9eee-45b4-ba47-44c2b3cc4c8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45539, 50), (45539,))"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "X=np.array(X)\n",
        "y=np.array(y)\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fYxo9IEjvLyR"
      },
      "source": [
        "Use the `to_categorical` function again to convert both `X` and `y` to one-hot encoded vectors of 0's and 1's, and check their shapes again. You should now have shapes (45539,50,29) and (45539,29). In other words, the vector `X` now contains 46,738 sequences of length 50, and each element of each sequence is a 30-dimensional vector of 29 zeros and a single one in the entry corresponding to some character in the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJi_IWtcUJZZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91bb8365-5b1b-4e60-cc2c-753a57b79359"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((45539, 50, 29), (45539, 29))"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ],
      "source": [
        "X = to_categorical(X, num_classes=n)\n",
        "y = to_categorical(y, num_classes=n)\n",
        "X.shape, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9aYcAmwwncs"
      },
      "source": [
        "You're now ready to create your model. Create a neural network called `model`. This should have an input layer, a recurrent layer with 128 neurons, a dense layer, and a softmax layer. For your recurrent layer, you can use SimpleRNN, or something more sophsticated like an LSTM. (You'll get better results with  LSTM, but it will take MUCH longer. You can mitigate this by reducing the length of each sequence in X down to 10.) The number of neurons in your dense layer should be appropriate to predict the categorical variable `y`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0RbPYB0UmBp"
      },
      "outputs": [],
      "source": [
        "model = Sequential()\n",
        "model.add(InputLayer(input_shape=(50, len(tok.tokens))))\n",
        "model.add(SimpleRNN(128))\n",
        "model.add(Dense(len(tok.tokens)))\n",
        "model.add(Softmax())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "dakQyasw51yl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a60199d-b640-45ba-e9aa-c3d5acc130af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_3 (SimpleRNN)    (None, 128)               20224     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 29)                3741      \n",
            "                                                                 \n",
            " softmax_3 (Softmax)         (None, 29)                0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 23,965\n",
            "Trainable params: 23,965\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iqIfJNyx1xL"
      },
      "source": [
        "Compile your model using the `Adam` optimizer and an approporiately chosen loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfvSeb5cU8nj"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GIl_LiFx-zD"
      },
      "source": [
        "Fit your data to X and y. Train for 50 epochs with a batch size of 128. Each epoch will take about 95 seconds, so you'll want to leave your computer for about an hour for this to complete."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9HMXgjxKVJb2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67290227-6aed-41c9-86f0-e5d18b0f1d6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "356/356 [==============================] - 16s 40ms/step - loss: 2.5113\n",
            "Epoch 2/50\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 2.1865\n",
            "Epoch 3/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 2.0766\n",
            "Epoch 4/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 2.0080\n",
            "Epoch 5/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.9585\n",
            "Epoch 6/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.9146\n",
            "Epoch 7/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.8769\n",
            "Epoch 8/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.8404\n",
            "Epoch 9/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.8092\n",
            "Epoch 10/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.7800\n",
            "Epoch 11/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.7506\n",
            "Epoch 12/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.7261\n",
            "Epoch 13/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.7024\n",
            "Epoch 14/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.6780\n",
            "Epoch 15/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.6586\n",
            "Epoch 16/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.6389\n",
            "Epoch 17/50\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.6185\n",
            "Epoch 18/50\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.5999\n",
            "Epoch 19/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.5798\n",
            "Epoch 20/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.5661\n",
            "Epoch 21/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.5508\n",
            "Epoch 22/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.5333\n",
            "Epoch 23/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.5196\n",
            "Epoch 24/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.5060\n",
            "Epoch 25/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.4928\n",
            "Epoch 26/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.4766\n",
            "Epoch 27/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.4640\n",
            "Epoch 28/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.4529\n",
            "Epoch 29/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.4414\n",
            "Epoch 30/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.4282\n",
            "Epoch 31/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.4208\n",
            "Epoch 32/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.4093\n",
            "Epoch 33/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.3982\n",
            "Epoch 34/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.3871\n",
            "Epoch 35/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.3772\n",
            "Epoch 36/50\n",
            "356/356 [==============================] - 14s 39ms/step - loss: 1.3696\n",
            "Epoch 37/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.3622\n",
            "Epoch 38/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.3550\n",
            "Epoch 39/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.3461\n",
            "Epoch 40/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.3388\n",
            "Epoch 41/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.3292\n",
            "Epoch 42/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.3225\n",
            "Epoch 43/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.3161\n",
            "Epoch 44/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.3113\n",
            "Epoch 45/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.3074\n",
            "Epoch 46/50\n",
            "356/356 [==============================] - 13s 37ms/step - loss: 1.2989\n",
            "Epoch 47/50\n",
            "356/356 [==============================] - 14s 38ms/step - loss: 1.2920\n",
            "Epoch 48/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.2861\n",
            "Epoch 49/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.2816\n",
            "Epoch 50/50\n",
            "356/356 [==============================] - 13s 38ms/step - loss: 1.2774\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f610df96cd0>"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "model.fit(X, y, epochs=50, batch_size=128)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQcsGH5Jyb7e"
      },
      "source": [
        "We will now use your trained model to generate text, one character at a time. Run the following code block to do this. (It will take a minute or two to complete.) Its interesting that although the model generates one character at a time, you'll see very word-like strings in the final text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t4IrfYOGaVHn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c454cf2a-2d70-4b62-d5df-4be2fad07a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 137ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 50ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 58ms/step\n",
            "1/1 [==============================] - 0s 40ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 38ms/step\n",
            "1/1 [==============================] - 0s 53ms/step\n",
            "1/1 [==============================] - 0s 41ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 37ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 39ms/step\n",
            "1/1 [==============================] - 0s 31ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 30ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 29ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' yem that if it make you wont gotagetreatlersew and and weve  jo tail  in then and soas was upon you'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ],
      "source": [
        "seq=[np.random.randint(0,len(tok.tokens)) for i in range(50)] #50 random integers for inital prediction\n",
        "seq=to_categorical(np.array(seq),num_classes=len(tok.tokens)) #one-hot encode initial sequence\n",
        "\n",
        "newtext=''\n",
        "for i in range(100):\n",
        "  pred_probs=model.predict(seq.reshape(1,50,len(tok.tokens))) #Use model to generate probs for next char\n",
        "  index_pred=np.random.choice(n,1,p=pred_probs.reshape(n))[0] #choose one\n",
        "  newtext+=tok.decode(index_pred) #corresponding character\n",
        "  seq=np.vstack([seq,to_categorical(index_pred,num_classes=len(tok.tokens))]) #add element to end of sequence\n",
        "  seq=seq[1:] #remove 1st element from sequence so we have another sequence of length 50\n",
        "\n",
        "newtext #display generated text"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nk9dRoo_0Ok_"
      },
      "source": [
        "**COPY AND PASTE THIS TEXT INTO THE SUBMISSION WINDOW ON GRADESCOPE**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}