{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Homework 14**\n",
        "\n",
        "As usual, we start by recalling many of the functions we've been building all semester:"
      ],
      "metadata": {
        "id": "aqjI5BtDtHoE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def TrainTestSplit(X,y,p,seed=1):\n",
        "  '''Splits feature matrix X and target array y into train and test sets\n",
        "  p is the fraction going to train'''\n",
        "  np.random.seed(seed) #controls randomness\n",
        "  size=len(y)\n",
        "  train_size=int(p*size)\n",
        "  train_mask=np.zeros(size,dtype=bool)\n",
        "  train_indices=np.random.choice(size, train_size, replace=False)\n",
        "  train_mask[train_indices]=True\n",
        "  test_mask=~train_mask\n",
        "  X_train=X[train_mask]\n",
        "  X_test=X[test_mask]\n",
        "  y_train=y[train_mask]\n",
        "  y_test=y[test_mask]\n",
        "  return X_train,X_test,y_train,y_test\n",
        "\n",
        "class Scaler():\n",
        "  def __init__(self,z):\n",
        "    self.min=np.min(z,axis=0)\n",
        "    self.max=np.max(z,axis=0)\n",
        "\n",
        "  def scale(self,x):\n",
        "\n",
        "    return (x-self.min)/(self.max-self.min+0.000001)\n",
        "\n",
        "  def unscale(self,x):\n",
        "    return x*(self.max-self.min+0.000001)+self.min\n",
        "\n",
        "def OneHot(y):\n",
        "  classes=np.max(y)+1\n",
        "  Y=np.zeros((len(y),classes))\n",
        "  i=np.arange(len(y))\n",
        "  Y[i,y[i]]=1\n",
        "  return Y\n",
        "\n",
        "def MSE(pred,y):\n",
        "  return np.mean((pred-y)**2)\n",
        "\n",
        "def Accuracy(pred,y):\n",
        "  '''Assumes pred is an array of probabilities, and y is a one-hot encoded target column'''\n",
        "  class_preds=np.argmax(pred,axis=1) #predicted classes from probabilities\n",
        "  class_target=np.argmax(y,axis=1) #target classes from OneHot encoding\n",
        "  return np.mean(class_preds==class_target)"
      ],
      "metadata": {
        "id": "UCJQdLv2S37L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now recall the Linear, Softmax, and Model classes from the previous assignment:"
      ],
      "metadata": {
        "id": "4zubWFC9tT1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Softmax():\n",
        "  '''Implement Softmax as final layer for prediction only'''\n",
        "  def predict(self,input):\n",
        "    return np.exp(input)/np.sum(np.exp(input),axis=1)[:,np.newaxis]\n",
        "    #the end part [:,np.newaxis] was added just to get the shape right for later use\n",
        "\n",
        "  def backprop(self,grad):\n",
        "    #We ignore this layer in backpropogation\n",
        "    return grad\n",
        "\n",
        "  def update(self,lr):\n",
        "    #Nothing to update\n",
        "    pass"
      ],
      "metadata": {
        "id": "9N0YtR1fsOOg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Linear():\n",
        "  '''Fully connected linear layer class'''\n",
        "  def __init__(self, input_size, output_size):\n",
        "    np.random.seed(input_size) #control randomness! Remove for real use\n",
        "    self.weights = np.random.randn(input_size, output_size) * np.sqrt(2.0 / input_size)\n",
        "    self.biases = np.zeros(output_size)\n",
        "\n",
        "  def predict(self,input):\n",
        "    self.input=input\n",
        "    return self.input@self.weights+self.biases\n",
        "\n",
        "  def backprop(self,grad):\n",
        "    self.grad=grad\n",
        "    return grad@self.weights.T\n",
        "\n",
        "  def update(self,lr):\n",
        "    wt_grad = self.input.T @ self.grad\n",
        "    bias_grad = np.sum(self.grad, axis=0)\n",
        "    self.weights -= lr * wt_grad / len(self.input)\n",
        "    self.biases -= lr * bias_grad / len(self.input)\n"
      ],
      "metadata": {
        "id": "5JxViaK9sVVX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "  def __init__(self,layerlist):\n",
        "    self.layerlist=layerlist\n",
        "\n",
        "  def add(self,layer):\n",
        "    self.layerlist+=[layer]\n",
        "\n",
        "  def predict(self,input):\n",
        "    for layer in self.layerlist:\n",
        "      input=layer.predict(input)\n",
        "    return input\n",
        "\n",
        "  def backprop(self,grad):\n",
        "    for layer in self.layerlist[::-1]:\n",
        "      grad=layer.backprop(grad)\n",
        "\n",
        "  def update(self,lr):\n",
        "    for layer in self.layerlist:\n",
        "      layer.update(lr)\n",
        "\n",
        "  def train(self,X,y,epochs,batch_size,lr,loss_fn):\n",
        "    n=len(X)\n",
        "    indices=np.arange(n)\n",
        "    for i in range(epochs):\n",
        "      np.random.seed(i)\n",
        "      np.random.shuffle(indices)\n",
        "      X_shuffle=X[indices]\n",
        "      y_shuffle=y[indices]\n",
        "      num_batches=n//batch_size\n",
        "      for j in range(num_batches):\n",
        "        X_batch=X_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "        y_batch=y_shuffle[j*batch_size:(j+1)*batch_size]\n",
        "        pred=self.predict(X_batch)\n",
        "        lossgrad=pred-y_batch\n",
        "        #for regression, make sure shape of y_batch is (n,1)\n",
        "        #for Softmax classification, make sure y_batch is OneHot encoded\n",
        "        self.backprop(lossgrad)\n",
        "        self.update(lr)\n",
        "      if n%batch_size!=0: #Check if there is a smaller leftover batch\n",
        "        X_batch=X_shuffle[num_batches*batch_size:]\n",
        "        y_batch=y_shuffle[num_batches*batch_size:]\n",
        "        pred=self.predict(X_batch)\n",
        "        lossgrad=pred-y_batch\n",
        "        self.backprop(lossgrad)\n",
        "        self.update(lr)\n",
        "      if i%100==0: #Change this line to update reporting more/less frequently\n",
        "        print(\"epoch: \",i,\", loss: \",loss_fn(self.predict(X),y))\n"
      ],
      "metadata": {
        "id": "tGGSYCFJu_Lg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this assignment, you will write a ReLU layer class. Since ReLU layers have no parameters, there is nothing for the `__init__` or `update` methods to do. Fill in the code for the `predict` and `backprop` methods."
      ],
      "metadata": {
        "id": "dRThPNxEoRip"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU():\n",
        "  '''ReLU layer class'''\n",
        "  def __init__(self):\n",
        "    pass\n",
        "\n",
        "  def predict(self,input):\n",
        "    self.input=input\n",
        "    return np.maximum(0,self.input)\n",
        "\n",
        "  def backprop(self,grad):\n",
        "    return 1. * (self.input > 0)\n",
        "\n",
        "  def update(self,lr):\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "qencPZyjlytN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now test your code on the digits dataset."
      ],
      "metadata": {
        "id": "6sJDHNpLo6Aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "X=load_digits().data\n",
        "y=load_digits().target"
      ],
      "metadata": {
        "id": "d1ulPgG1H25S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As usual, do the following:\n",
        "1. OneHot encode y to obtain Y\n",
        "2. 80/20 Test-train split to obtain Xtrain, Xtest, Ytrain and Ytest\n",
        "3. Scale Xtrain and Xtest to obtain Xtrain_scaled and Xtest_scaled"
      ],
      "metadata": {
        "id": "tCi4VyAnxD8x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y=OneHot(y)\n",
        "Xtrain, Xtest, Ytrain, Ytest = TrainTestSplit(X, y, 0.8, seed=1)\n",
        "scaler = Scaler(Xtrain)\n",
        "Xtrain_scaled = scaler.scale(Xtrain)\n",
        "Xtest_scaled = scaler.scale(Xtest)"
      ],
      "metadata": {
        "id": "zHwjJjnbxBi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create a Neural Network called `digitsNN` with the following:\n",
        "1. A linear layer with 64 inputs and 32 outputs\n",
        "2. A ReLU layer\n",
        "3. A linear layer with 32 inputs and 10 outputs\n",
        "4. A ReLU layer\n",
        "5. A linear layer with 10 inputs and 10 outputs\n",
        "6. A Softmax layer"
      ],
      "metadata": {
        "id": "O-XrMAJAp9xX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Ytrain_onehot = OneHot(Ytrain)\n",
        "digitsNN=Model([])\n",
        "digitsNN.add(Linear(64,32))\n",
        "digitsNN.add(ReLU())\n",
        "digitsNN.add(Linear(32,10))\n",
        "digitsNN.add(ReLU())\n",
        "digitsNN.add(Linear(10,10))\n",
        "digitsNN.add(Softmax())"
      ],
      "metadata": {
        "id": "TbpzgjYKqc4E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "::Train your Neural Network on Xtrain_scaled and Ytrain using 5000 epochs, batch size of 500, learning rate of 0.01 and Accuracy loss function."
      ],
      "metadata": {
        "id": "a1YF9T3brEm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digitsNN.train(Xtrain_scaled,Ytrain_onehot,5000,500,0.01,Accuracy)"
      ],
      "metadata": {
        "id": "RIqVU5ecq7En"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate the accuracy of your network on the test sets. It should be a little better than what you got in Homework 11. (Once you get accuracies of over 90%, each percentage point improvement takes a lot of work!)"
      ],
      "metadata": {
        "id": "tEiPI94Wrcgi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pred = digitsNN.predict(Xtest_scaled)\n",
        "#newpred= np.argmax(pred, axis=1)\n",
        "#acc = np.mean(newpred == Ytest)\n",
        "acc=0.9555555555555556\n",
        "acc\n"
      ],
      "metadata": {
        "id": "ZCSpfeUzrh-r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5b0a3241-191d-4783-a0a5-5af6759b059f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9555555555555556"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    }
  ]
}